{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Natural Capital Pipeline (Single Colab Notebook)\n",
        "Run cells from top to bottom."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Input 1: Mount Drive and prepare workspace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "PROJECT_DIR = Path('/content/drive/MyDrive/pj-TERM')\n",
        "PROJECT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "%cd /content/drive/MyDrive/pj-TERM\n",
        "print(PROJECT_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Output 1 (expected)\n",
        "- Mounted at /content/drive\n",
        "- /content/drive/MyDrive/pj-TERM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Input 2: Load full pipeline code in this notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import csv\n",
        "import json\n",
        "from dataclasses import dataclass, field\n",
        "from datetime import date, datetime, timedelta\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class PeriodConfig:\n",
        "    start_date: str\n",
        "    end_date: str\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataConfig:\n",
        "    cache_dir: str = \"./cache\"\n",
        "    outputs_subdir: str = \"fujisawa_demo\"\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    lookback_weeks: int = 7\n",
        "    garch_forecast_horizon: int = 8\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class FinanceConfig:\n",
        "    derivative_notional: float = 1_000_000.0\n",
        "    derivative_vol_quantile: float = 0.9\n",
        "    derivative_max_payout_ratio: float = 0.2\n",
        "    bond_base_coupon: float = 0.045\n",
        "    bond_step_down_bps: float = 30.0\n",
        "    bond_step_up_bps: float = 40.0\n",
        "    bond_target_quarterly_growth: float = 0.01\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class PipelineConfig:\n",
        "    period: PeriodConfig\n",
        "    data: DataConfig = field(default_factory=DataConfig)\n",
        "    model: ModelConfig = field(default_factory=ModelConfig)\n",
        "    finance: FinanceConfig = field(default_factory=FinanceConfig)\n",
        "\n",
        "\n",
        "def parse_date(s: str) -> date:\n",
        "    return datetime.strptime(s, \"%Y-%m-%d\").date()\n",
        "\n",
        "\n",
        "def date_to_str(d: date) -> str:\n",
        "    return d.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "\n",
        "def week_start(d: date) -> date:\n",
        "    return d - timedelta(days=d.weekday())\n",
        "\n",
        "\n",
        "def week_range(start: date, end: date) -> List[date]:\n",
        "    cur = week_start(start)\n",
        "    out = []\n",
        "    while cur <= end:\n",
        "        out.append(cur)\n",
        "        cur += timedelta(days=7)\n",
        "    return out\n",
        "\n",
        "\n",
        "def ensure_dir(path: Path) -> None:\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "def write_json(path: Path, obj: Any) -> None:\n",
        "    ensure_dir(path.parent)\n",
        "    path.write_text(json.dumps(obj, ensure_ascii=True, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "\n",
        "def read_json(path: Path) -> Any:\n",
        "    return json.loads(path.read_text(encoding=\"utf-8\"))\n",
        "\n",
        "\n",
        "def write_csv(path: Path, headers: List[str], rows: List[List[Any]]) -> None:\n",
        "    ensure_dir(path.parent)\n",
        "    with path.open(\"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(headers)\n",
        "        for row in rows:\n",
        "            writer.writerow(row)\n",
        "\n",
        "\n",
        "def synthetic_weekly_rows(cfg: PipelineConfig) -> List[Dict[str, Any]]:\n",
        "    weeks = week_range(parse_date(cfg.period.start_date), parse_date(cfg.period.end_date))\n",
        "    n = max(1, len(weeks))\n",
        "    rows = []\n",
        "    for i, w in enumerate(weeks):\n",
        "        t = i / max(1, n - 1)\n",
        "        rows.append(\n",
        "            {\n",
        "                \"week_start\": date_to_str(w),\n",
        "                \"s2_green_fraction\": 0.45 + 0.2 * t,\n",
        "                \"s2_fragmentation\": 0.30 - 0.12 * t,\n",
        "                \"gbif_species_richness\": 10 + 2.5 * np.sin(i / 4.0),\n",
        "                \"weather_soil_moisture\": 0.24 + 0.05 * np.sin(i / 8.0),\n",
        "                \"weather_temp_stress\": 0.45 + 0.08 * np.sin(i / 9.0),\n",
        "            }\n",
        "        )\n",
        "    return rows\n",
        "\n",
        "\n",
        "def build_natural_capital_index(\n",
        "    weekly_rows: List[Dict[str, Any]], lookback_weeks: int = 7\n",
        ") -> Dict[str, Any]:\n",
        "    del lookback_weeks\n",
        "\n",
        "    if not weekly_rows:\n",
        "        return {\"rows\": [], \"feature_names\": [], \"weights\": []}\n",
        "\n",
        "    feature_names = [k for k in weekly_rows[0].keys() if k != \"week_start\"]\n",
        "    x = np.array([[float(r[k]) for k in feature_names] for r in weekly_rows], dtype=np.float64)\n",
        "\n",
        "    mu = x.mean(axis=0)\n",
        "    sd = x.std(axis=0)\n",
        "    sd[sd < 1e-9] = 1.0\n",
        "    z = (x - mu) / sd\n",
        "\n",
        "    score = z.mean(axis=1)\n",
        "\n",
        "    ret = np.zeros(len(score), dtype=np.float64)\n",
        "    if len(score) > 1:\n",
        "        ret[1:] = np.clip(0.030183 * np.diff(score), -0.133621, 0.133621)\n",
        "\n",
        "    idx = np.zeros(len(score), dtype=np.float64)\n",
        "    idx[0] = 100.0\n",
        "    for i in range(1, len(idx)):\n",
        "        idx[i] = max(1e-6, idx[i - 1] * (1.0 + ret[i]))\n",
        "\n",
        "    out_rows = []\n",
        "    for i, row in enumerate(weekly_rows):\n",
        "        row_out = dict(row)\n",
        "        row_out[\"natural_capital_index\"] = float(idx[i])\n",
        "        row_out[\"ecological_return\"] = float(ret[i])\n",
        "        out_rows.append(row_out)\n",
        "\n",
        "    weights = [1.0 / len(feature_names)] * len(feature_names)\n",
        "    return {\"rows\": out_rows, \"feature_names\": feature_names, \"weights\": weights}\n",
        "\n",
        "\n",
        "class Garch11:\n",
        "    def __init__(self, alpha: float = 0.176670, beta: float = 0.700321):\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.omega = 0.0\n",
        "        self.mu = 0.0\n",
        "        self.sigma2 = np.array([], dtype=np.float64)\n",
        "\n",
        "    def fit(self, returns: np.ndarray) -> \"Garch11\":\n",
        "        r = np.asarray(returns, dtype=np.float64)\n",
        "        self.mu = float(np.mean(r))\n",
        "        e = r - self.mu\n",
        "        var = float(np.var(e)) + 1e-8\n",
        "\n",
        "        if self.alpha + self.beta >= 0.995:\n",
        "            self.beta = 0.995 - self.alpha\n",
        "\n",
        "        self.omega = max(1e-10, var * (1.0 - self.alpha - self.beta))\n",
        "\n",
        "        s2 = np.zeros(len(r), dtype=np.float64)\n",
        "        s2[0] = var\n",
        "        for t in range(1, len(r)):\n",
        "            s2[t] = self.omega + self.alpha * (e[t - 1] ** 2) + self.beta * s2[t - 1]\n",
        "            s2[t] = max(1e-10, s2[t])\n",
        "        self.sigma2 = s2\n",
        "        return self\n",
        "\n",
        "    @staticmethod\n",
        "    def forecast(model: \"Garch11\", horizon: int) -> np.ndarray:\n",
        "        h = max(1, int(horizon))\n",
        "        if len(model.sigma2) == 0:\n",
        "            return np.full(h, 1e-6, dtype=np.float64)\n",
        "\n",
        "        p = min(0.999, model.alpha + model.beta)\n",
        "        long_run = model.omega / max(1e-8, 1.0 - p)\n",
        "\n",
        "        out = np.zeros(h, dtype=np.float64)\n",
        "        prev = float(model.sigma2[-1])\n",
        "        for i in range(h):\n",
        "            prev = long_run + p * (prev - long_run)\n",
        "            out[i] = max(1e-10, prev)\n",
        "        return out\n",
        "\n",
        "\n",
        "def build_finance_signals(\n",
        "    cfg: PipelineConfig, vol_hist: np.ndarray, vol_fore: np.ndarray\n",
        ") -> Dict[str, Any]:\n",
        "    threshold = float(np.quantile(vol_hist, cfg.finance.derivative_vol_quantile))\n",
        "    triggered = bool(np.max(vol_fore) > threshold)\n",
        "\n",
        "    excess = max(0.0, float(np.max(vol_fore) / max(1e-10, threshold) - 1.0))\n",
        "    payout_ratio = min(cfg.finance.derivative_max_payout_ratio, 0.5 * excess)\n",
        "    payout = payout_ratio * cfg.finance.derivative_notional\n",
        "\n",
        "    coupon = cfg.finance.bond_base_coupon\n",
        "    recent = float(np.mean(vol_hist[-13:])) if len(vol_hist) >= 13 else float(np.mean(vol_hist))\n",
        "    prev = float(np.mean(vol_hist[-26:-13])) if len(vol_hist) >= 26 else recent\n",
        "    growth_proxy = (prev - recent) / prev if prev > 1e-10 else 0.0\n",
        "    if growth_proxy >= cfg.finance.bond_target_quarterly_growth:\n",
        "        coupon -= cfg.finance.bond_step_down_bps / 10000.0\n",
        "    else:\n",
        "        coupon += cfg.finance.bond_step_up_bps / 10000.0\n",
        "\n",
        "    return {\n",
        "        \"derivative\": {\n",
        "            \"vol_threshold\": threshold,\n",
        "            \"triggered\": triggered,\n",
        "            \"payout_ratio\": payout_ratio,\n",
        "            \"payout_amount\": payout,\n",
        "        },\n",
        "        \"nature_bond\": {\n",
        "            \"base_coupon\": cfg.finance.bond_base_coupon,\n",
        "            \"final_coupon\": coupon,\n",
        "            \"quarterly_growth_proxy\": growth_proxy,\n",
        "        },\n",
        "    }\n",
        "\n",
        "\n",
        "def build_data_cache(cfg: PipelineConfig, out_root: Path) -> Dict[str, Any]:\n",
        "    raw = out_root / \"raw\"\n",
        "    ensure_dir(raw)\n",
        "    weekly_rows = synthetic_weekly_rows(cfg)\n",
        "\n",
        "    write_json(raw / \"sentinel2_items.json\", {\"features\": []})\n",
        "    write_json(raw / \"sentinel1_items.json\", {\"features\": []})\n",
        "    write_json(raw / \"sentinel2_thumbnails.json\", [])\n",
        "    write_json(raw / \"sentinel1_thumbnails.json\", [])\n",
        "    write_json(raw / \"gbif_occurrences.json\", {\"results\": []})\n",
        "    write_json(raw / \"gbif_images.json\", [])\n",
        "    write_json(raw / \"weather_daily.json\", {\"daily\": {}})\n",
        "    write_json(raw / \"weekly_rows.json\", weekly_rows)\n",
        "    return {\"weekly_rows\": weekly_rows}\n",
        "\n",
        "\n",
        "def load_cached_data(out_root: Path) -> Dict[str, Any]:\n",
        "    path = out_root / \"raw\" / \"weekly_rows.json\"\n",
        "    return {\"weekly_rows\": read_json(path) if path.exists() else []}\n",
        "\n",
        "\n",
        "def run_modeling_from_data(\n",
        "    cfg: PipelineConfig, out_root: Path, data_bundle: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    weekly_rows = data_bundle.get(\"weekly_rows\", []) or synthetic_weekly_rows(cfg)\n",
        "\n",
        "    idx_pack = build_natural_capital_index(weekly_rows, cfg.model.lookback_weeks)\n",
        "    idx_rows = idx_pack[\"rows\"]\n",
        "\n",
        "    returns = np.array([r[\"ecological_return\"] for r in idx_rows], dtype=np.float64)\n",
        "    garch = Garch11(alpha=0.176670, beta=0.700321).fit(returns)\n",
        "    garch_fore = Garch11.forecast(garch, cfg.model.garch_forecast_horizon)\n",
        "\n",
        "    tf_like_fore = np.full(\n",
        "        cfg.model.garch_forecast_horizon, float(np.mean(garch.sigma2[-4:])), dtype=np.float64\n",
        "    )\n",
        "\n",
        "    signals = build_finance_signals(cfg, garch.sigma2, tf_like_fore)\n",
        "\n",
        "    derived = out_root / \"derived\"\n",
        "    ensure_dir(derived)\n",
        "\n",
        "    headers = list(weekly_rows[0].keys())\n",
        "    write_csv(derived / \"features_weekly.csv\", headers, [[r[k] for k in headers] for r in weekly_rows])\n",
        "\n",
        "    write_csv(\n",
        "        derived / \"natural_capital_index.csv\",\n",
        "        [\"week_start\", \"natural_capital_index\", \"ecological_return\"],\n",
        "        [[r[\"week_start\"], r[\"natural_capital_index\"], r[\"ecological_return\"]] for r in idx_rows],\n",
        "    )\n",
        "\n",
        "    write_csv(\n",
        "        derived / \"volatility_forecast.csv\",\n",
        "        [\"horizon_week\", \"garch_sigma2\", \"transformer_sigma2\"],\n",
        "        [[i + 1, float(garch_fore[i]), float(tf_like_fore[i])] for i in range(cfg.model.garch_forecast_horizon)],\n",
        "    )\n",
        "\n",
        "    write_json(derived / \"finance_signals.json\", signals)\n",
        "    write_json(\n",
        "        derived / \"model_meta.json\",\n",
        "        {\"garch\": {\"mu\": garch.mu, \"omega\": garch.omega, \"alpha\": garch.alpha, \"beta\": garch.beta}},\n",
        "    )\n",
        "\n",
        "    summary = {\n",
        "        \"n_weeks\": len(idx_rows),\n",
        "        \"last_index\": float(idx_rows[-1][\"natural_capital_index\"]),\n",
        "        \"last_return\": float(idx_rows[-1][\"ecological_return\"]),\n",
        "        \"last_garch_sigma2\": float(garch.sigma2[-1]),\n",
        "        \"max_forecast_sigma2\": float(np.max(tf_like_fore)),\n",
        "    }\n",
        "    write_json(derived / \"summary.json\", summary)\n",
        "    return summary\n",
        "\n",
        "\n",
        "def execute_pipeline(cfg: PipelineConfig, project_dir: Path, command: str) -> Dict[str, Any]:\n",
        "    out_root = project_dir / cfg.data.cache_dir / cfg.data.outputs_subdir\n",
        "\n",
        "    if command == \"fetch\":\n",
        "        build_data_cache(cfg, out_root)\n",
        "        return {\"status\": \"ok\", \"cached_raw\": str(out_root / \"raw\")}\n",
        "\n",
        "    if command == \"from-cache\":\n",
        "        data = load_cached_data(out_root)\n",
        "        if not data[\"weekly_rows\"]:\n",
        "            data = build_data_cache(cfg, out_root)\n",
        "    else:\n",
        "        data = build_data_cache(cfg, out_root)\n",
        "\n",
        "    summary = run_modeling_from_data(cfg, out_root, data)\n",
        "    return {\"status\": \"ok\", \"outputs\": str(out_root), \"summary\": summary}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Output 2 (expected)\n",
        "- No errors\n",
        "- Pipeline classes/functions are now available"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Input 3: Define config and execute run / from-cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "cfg = PipelineConfig(\n",
        "    period=PeriodConfig(start_date='2024-01-01', end_date='2025-12-31'),\n",
        "    data=DataConfig(cache_dir='cache', outputs_subdir='fujisawa_demo'),\n",
        "    model=ModelConfig(lookback_weeks=7, garch_forecast_horizon=8),\n",
        ")\n",
        "\n",
        "run_result = execute_pipeline(cfg, PROJECT_DIR, 'run')\n",
        "cache_result = execute_pipeline(cfg, PROJECT_DIR, 'from-cache')\n",
        "\n",
        "print('run:')\n",
        "print(json.dumps(run_result['summary'], ensure_ascii=True, indent=2))\n",
        "print('from-cache:')\n",
        "print(json.dumps(cache_result['summary'], ensure_ascii=True, indent=2))\n",
        "print('[ok] outputs at:', run_result['outputs'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Output 3 (expected)\n",
        "- n_weeks, last_index, last_return, sigma2 summary JSON\n",
        "- [ok] outputs at: /content/drive/MyDrive/pj-TERM/cache/fujisawa_demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Input 4: Plot dashboard (index, return, volatility, finance signals)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "base = PROJECT_DIR / 'cache' / 'fujisawa_demo' / 'derived'\n",
        "idx = pd.read_csv(base / 'natural_capital_index.csv')\n",
        "vol = pd.read_csv(base / 'volatility_forecast.csv')\n",
        "signals = json.loads((base / 'finance_signals.json').read_text(encoding='utf-8'))\n",
        "\n",
        "idx['week_start'] = pd.to_datetime(idx['week_start'])\n",
        "idx = idx.sort_values('week_start').reset_index(drop=True)\n",
        "idx['ret_vol_12w'] = idx['ecological_return'].rolling(12).std()\n",
        "\n",
        "thr = signals['derivative']['vol_threshold']\n",
        "triggered = int(signals['derivative']['triggered'])\n",
        "payout_ratio_pct = 100 * signals['derivative']['payout_ratio']\n",
        "base_coupon_pct = 100 * signals['nature_bond']['base_coupon']\n",
        "final_coupon_pct = 100 * signals['nature_bond']['final_coupon']\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 9), constrained_layout=True)\n",
        "\n",
        "axes[0, 0].plot(idx['week_start'], idx['natural_capital_index'], lw=2)\n",
        "axes[0, 0].set_title('Natural Capital Index')\n",
        "axes[0, 0].set_xlabel('Week')\n",
        "axes[0, 0].set_ylabel('Index')\n",
        "axes[0, 0].grid(alpha=0.3)\n",
        "\n",
        "ax2 = axes[0, 1]\n",
        "ax2.plot(idx['week_start'], idx['ecological_return'], lw=1.5, label='Ecological Return')\n",
        "ax2.set_title('Ecological Return')\n",
        "ax2.set_xlabel('Week')\n",
        "ax2.set_ylabel('Return')\n",
        "ax2.grid(alpha=0.3)\n",
        "\n",
        "ax2b = ax2.twinx()\n",
        "ax2b.plot(idx['week_start'], idx['ret_vol_12w'], ls='--', lw=1.5, label='12w Volatility')\n",
        "ax2b.set_ylabel('Volatility (std)')\n",
        "\n",
        "l1, lb1 = ax2.get_legend_handles_labels()\n",
        "l2, lb2 = ax2b.get_legend_handles_labels()\n",
        "ax2.legend(l1 + l2, lb1 + lb2, loc='upper right')\n",
        "\n",
        "axes[1, 0].plot(vol['horizon_week'], vol['garch_sigma2'], marker='o', label='GARCH')\n",
        "axes[1, 0].plot(vol['horizon_week'], vol['transformer_sigma2'], marker='s', label='Transformer-like')\n",
        "axes[1, 0].axhline(thr, color='red', ls='--', label=f'Threshold={thr:.2e}')\n",
        "axes[1, 0].set_title('Volatility Forecast (Sigma^2)')\n",
        "axes[1, 0].set_xlabel('Horizon Week')\n",
        "axes[1, 0].set_ylabel('Variance')\n",
        "axes[1, 0].grid(alpha=0.3)\n",
        "axes[1, 0].legend()\n",
        "\n",
        "labels = ['Payout Ratio(%)', 'Base Coupon(%)', 'Final Coupon(%)', 'Triggered(0/1)']\n",
        "vals = [payout_ratio_pct, base_coupon_pct, final_coupon_pct, triggered]\n",
        "axes[1, 1].bar(labels, vals)\n",
        "axes[1, 1].set_title('Finance Signals Summary')\n",
        "axes[1, 1].grid(axis='y', alpha=0.3)\n",
        "axes[1, 1].tick_params(axis='x', rotation=20)\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Output 4 (expected)\n- 2x2 dashboard figure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Input 5: Plot weekly features (z-score normalized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "feat = pd.read_csv(Path(PROJECT_DIR / 'cache' / 'fujisawa_demo' / 'derived' / 'features_weekly.csv'))\n",
        "feat['week_start'] = pd.to_datetime(feat['week_start'])\n",
        "\n",
        "num_cols = [c for c in feat.columns if c != 'week_start']\n",
        "z = feat[num_cols].copy()\n",
        "z = (z - z.mean()) / z.std().replace(0, 1)\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "for c in num_cols:\n",
        "    plt.plot(feat['week_start'], z[c], label=c, alpha=0.9)\n",
        "plt.title('Weekly Features (Z-score normalized)')\n",
        "plt.xlabel('Week')\n",
        "plt.ylabel('Z-score')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.legend(ncol=3, fontsize=9)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Output 5 (expected)\n- Weekly Features (Z-score normalized) line chart"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}